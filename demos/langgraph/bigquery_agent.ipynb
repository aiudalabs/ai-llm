{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf57e16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eda2779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01fb894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Any, Annotated\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "\"\"\"model_name = \"gpt-4o\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\", \n",
    "    model=model_name, temperature=0, streaming=True\n",
    ")\"\"\"\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model_name=model_name,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "# Configuración del cliente BigQuery\n",
    "def get_bigquery_client(\n",
    "    credentials_path: str = \"/Users/nmlemus/.config/gcloud/application_default_credentials.json\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Inicializa y retorna un cliente de BigQuery.\n",
    "    \"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        credentials_path, scopes=[\"https://www.googleapis.com/auth/bigquery\"]\n",
    "    )\n",
    "    return bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "\n",
    "# Modelos de datos para el estado del grafo\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"Estado del agente LangGraph.\"\"\"\n",
    "\n",
    "    messages: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    database_schema: Optional[Dict[str, Any]] = None\n",
    "    user_question: Optional[str] = None\n",
    "    sql_query: Optional[str] = None\n",
    "    query_result: Optional[Dict[str, Any]] = None\n",
    "    needs_clarification: bool = False\n",
    "    clarification_message: Optional[str] = None\n",
    "\n",
    "\n",
    "# Nodos del grafo\n",
    "class Nodes(str, Enum):\n",
    "    EXTRACT_SCHEMA = \"extract_schema\"\n",
    "    VALIDATE_QUESTION = \"validate_question\"\n",
    "    GENERATE_QUERY = \"generate_query\"\n",
    "    VALIDATE_QUERY = \"validate_query\"\n",
    "    EXECUTE_QUERY = \"execute_query\"\n",
    "    PRESENT_RESULTS = \"present_results\"\n",
    "    REQUEST_CLARIFICATION = \"request_clarification\"\n",
    "\n",
    "\n",
    "# Funciones para los nodos del grafo\n",
    "def extract_schema(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Extrae el esquema de la base de datos de BigQuery.\n",
    "    \"\"\"\n",
    "    client = get_bigquery_client()\n",
    "\n",
    "    # Aquí necesitamos definir el dataset a consultar\n",
    "    dataset_id = \"your_dataset_id\"  # Reemplazar con el ID de tu dataset\n",
    "\n",
    "    schema = {}\n",
    "\n",
    "    # Obtener todas las tablas en el dataset\n",
    "    tables = list(client.list_tables(dataset_id))\n",
    "\n",
    "    for table in tables:\n",
    "        table_id = f\"{client.project}.{dataset_id}.{table.table_id}\"\n",
    "        table_ref = client.get_table(table_id)\n",
    "\n",
    "        # Guardar la estructura de la tabla\n",
    "        schema[table.table_id] = {\n",
    "            \"columns\": [\n",
    "                {\n",
    "                    \"name\": field.name,\n",
    "                    \"type\": field.field_type,\n",
    "                    \"description\": field.description,\n",
    "                }\n",
    "                for field in table_ref.schema\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # Actualizar el estado con el esquema extraído\n",
    "    state.database_schema = schema\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def validate_question(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Valida si la pregunta del usuario tiene sentido según la estructura de la base de datos.\n",
    "    \"\"\"\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Extraer la pregunta del usuario\n",
    "    for message in reversed(state.messages):\n",
    "        if message[\"role\"] == \"user\":\n",
    "            state.user_question = message[\"content\"]\n",
    "            break\n",
    "\n",
    "    # Prompt para validar la pregunta\n",
    "    prompt = f\"\"\"\n",
    "    Actúa como un experto analista de datos.\n",
    "    \n",
    "    Tienes acceso a una base de datos con las siguientes tablas y columnas:\n",
    "    {json.dumps(state.database_schema, indent=2)}\n",
    "    \n",
    "    La pregunta del usuario es: \"{state.user_question}\"\n",
    "    \n",
    "    Evalúa si esta pregunta puede ser respondida con la estructura de base de datos proporcionada.\n",
    "    Para que una pregunta sea válida:\n",
    "    1. Debe hacer referencia a datos que estén dentro de las tablas disponibles\n",
    "    2. Los campos que menciona deben existir en las tablas\n",
    "    3. La relación entre las tablas debe ser posible (si la pregunta involucra múltiples tablas)\n",
    "    \n",
    "    Responde con un JSON con el siguiente formato:\n",
    "    {{\n",
    "        \"is_valid\": true o false,\n",
    "        \"reason\": \"Explicación de por qué la pregunta es válida o no\",\n",
    "        \"clarification_needed\": \"Si se necesita aclaración, especifica qué información adicional se necesita\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    validation_result = json.loads(response.content)\n",
    "\n",
    "    # Actualizar el estado según el resultado de la validación\n",
    "    state.needs_clarification = not validation_result[\"is_valid\"]\n",
    "    if state.needs_clarification:\n",
    "        state.clarification_message = validation_result[\"clarification_needed\"]\n",
    "\n",
    "    # Determinar el siguiente nodo\n",
    "    if state.needs_clarification:\n",
    "        return {\"next\": Nodes.REQUEST_CLARIFICATION}\n",
    "    else:\n",
    "        return {\"next\": Nodes.GENERATE_QUERY}\n",
    "\n",
    "\n",
    "def generate_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Genera una consulta SQL basada en la pregunta del usuario y el esquema de la base de datos.\n",
    "    \"\"\"\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Prompt para generar la consulta SQL\n",
    "    prompt = f\"\"\"\n",
    "    Actúa como un experto en SQL para BigQuery.\n",
    "    \n",
    "    Tienes acceso a una base de datos con las siguientes tablas y columnas:\n",
    "    {json.dumps(state.database_schema, indent=2)}\n",
    "    \n",
    "    La pregunta del usuario es: \"{state.user_question}\"\n",
    "    \n",
    "    Genera una consulta SQL para BigQuery que responda a esta pregunta.\n",
    "    La consulta debe ser eficiente y utilizar las mejores prácticas de SQL.\n",
    "    Incluye comentarios para explicar cada parte relevante de la consulta.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Extraer la consulta SQL (asumiendo que está en formato de bloque de código)\n",
    "    content = response.content\n",
    "\n",
    "    # Extraer el código SQL de un bloque de código markdown si existe\n",
    "    import re\n",
    "\n",
    "    sql_match = re.search(r\"```sql\\n(.*?)\\n```\", content, re.DOTALL)\n",
    "\n",
    "    if sql_match:\n",
    "        state.sql_query = sql_match.group(1).strip()\n",
    "    else:\n",
    "        # Si no está en un bloque de código, intentamos buscar la consulta SQL directamente\n",
    "        state.sql_query = content.strip()\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def validate_query(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Valida si la consulta SQL es correcta y responde a la pregunta del usuario.\n",
    "    \"\"\"\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Prompt para validar la consulta SQL\n",
    "    prompt = f\"\"\"\n",
    "    Actúa como un experto en SQL para BigQuery y análisis de datos.\n",
    "    \n",
    "    La pregunta del usuario es: \"{state.user_question}\"\n",
    "    \n",
    "    La consulta SQL generada es:\n",
    "    ```sql\n",
    "    {state.sql_query}\n",
    "    ```\n",
    "    \n",
    "    El esquema de la base de datos es:\n",
    "    {json.dumps(state.database_schema, indent=2)}\n",
    "    \n",
    "    Evalúa si la consulta SQL:\n",
    "    1. Es sintácticamente correcta para BigQuery\n",
    "    2. Utiliza las tablas y columnas correctas del esquema proporcionado\n",
    "    3. Responde adecuadamente a la pregunta del usuario\n",
    "    \n",
    "    Responde con un JSON con el siguiente formato:\n",
    "    {{\n",
    "        \"is_valid\": true o false,\n",
    "        \"reason\": \"Explicación de por qué la consulta es válida o no\",\n",
    "        \"needs_clarification\": true o false,\n",
    "        \"clarification_message\": \"Si se necesita aclaración, especifica qué información adicional se necesita\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    validation_result = json.loads(response.content)\n",
    "\n",
    "    # Actualizar el estado según el resultado de la validación\n",
    "    if not validation_result[\"is_valid\"]:\n",
    "        if validation_result.get(\"needs_clarification\", False):\n",
    "            state.needs_clarification = True\n",
    "            state.clarification_message = validation_result[\"clarification_message\"]\n",
    "            return {\"next\": Nodes.REQUEST_CLARIFICATION}\n",
    "        else:\n",
    "            return {\"next\": Nodes.GENERATE_QUERY}\n",
    "    else:\n",
    "        return {\"next\": Nodes.EXECUTE_QUERY}\n",
    "\n",
    "\n",
    "def execute_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Ejecuta la consulta SQL en BigQuery y almacena los resultados.\n",
    "    \"\"\"\n",
    "    client = get_bigquery_client()\n",
    "\n",
    "    try:\n",
    "        # Ejecutar la consulta\n",
    "        query_job = client.query(state.sql_query)\n",
    "        results = query_job.result()\n",
    "\n",
    "        # Convertir los resultados a un formato serializable\n",
    "        rows = []\n",
    "        for row in results:\n",
    "            rows.append({key: value for key, value in row.items()})\n",
    "\n",
    "        # Guardar los resultados y metadatos\n",
    "        state.query_result = {\n",
    "            \"columns\": [field.name for field in query_job.result().schema],\n",
    "            \"rows\": rows,\n",
    "            \"total_rows\": len(rows),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # En caso de error, marcamos que necesitamos clarificación\n",
    "        state.needs_clarification = True\n",
    "        state.clarification_message = f\"Error al ejecutar la consulta: {str(e)}\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def present_results(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Formatea y presenta los resultados de la consulta al usuario.\n",
    "    \"\"\"\n",
    "    #llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    if state.query_result and state.query_result[\"total_rows\"] > 0:\n",
    "        # Limitar la cantidad de filas para no sobrecargar el mensaje\n",
    "        max_rows = min(10, state.query_result[\"total_rows\"])\n",
    "        display_rows = state.query_result[\"rows\"][:max_rows]\n",
    "        \n",
    "        # Crear una tabla Markdown con los resultados\n",
    "        columns = state.query_result[\"columns\"]\n",
    "        table_header = \"| \" + \" | \".join(columns) + \" |\"\n",
    "        table_separator = \"| \" + \" | \".join([\"---\" for _ in columns]) + \" |\"\n",
    "        \n",
    "        table_rows = []\n",
    "        for row in display_rows:\n",
    "            row_values = [str(row.get(col, \"\")) for col in columns]\n",
    "            table_rows.append(\"| \" + \" | \".join(row_values) + \" |\")\n",
    "        \n",
    "        table = \"\\n\".join([table_header, table_separator] + table_rows)\n",
    "        \n",
    "        # Prompt para generar una explicación de los resultados\n",
    "        prompt = f\"\"\"\n",
    "        Actúa como un analista de datos.\n",
    "        \n",
    "        La pregunta del usuario fue: \"{state.user_question}\"\n",
    "        \n",
    "        Los resultados de la consulta SQL son:\n",
    "        {table}\n",
    "        \n",
    "        Total de filas: {state.query_result[\"total_rows\"]}\n",
    "        (Mostrando {max_rows} de {state.query_result[\"total_rows\"]} filas)\n",
    "        \n",
    "        Proporciona una breve explicación de los resultados que responda directamente a la pregunta del usuario.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Crear un mensaje con los resultados y la explicación\n",
    "        result_message = f\"\"\"\n",
    "## Resultados de la consulta\n",
    "\n",
    "{table}\n",
    "\n",
    "*Mostrando {max_rows} de {state.query_result[\"total_rows\"]} filas*\n",
    "\n",
    "## Análisis\n",
    "\n",
    "{response.content}\n",
    "\n",
    "## Consulta SQL utilizada\n",
    "\n",
    "```sql\n",
    "{state.sql_query}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Añadir el mensaje al estado\n",
    "        state.messages.append({\"role\": \"assistant\", \"content\": result_message})\n",
    "    else:\n",
    "        # No hay resultados\n",
    "        no_results_message = f\"\"\"\n",
    "No se encontraron resultados para la consulta. Esto puede deberse a:\n",
    "1. No hay datos que coincidan con los criterios de la consulta\n",
    "2. La consulta puede necesitar ser ajustada\n",
    "\n",
    "Consulta SQL utilizada:\n",
    "```sql\n",
    "{state.sql_query}\n",
    "```\n",
    "\n",
    "¿Deseas reformular tu pregunta o ajustar los criterios de búsqueda?\n",
    "\"\"\"\n",
    "        state.messages.append({\"role\": \"assistant\", \"content\": no_results_message})\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def request_clarification(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Solicita aclaración al usuario basado en el mensaje de clarificación.\n",
    "    \"\"\"\n",
    "    clarification_request = f\"\"\"\n",
    "        Necesito un poco más de información para poder responder a tu pregunta correctamente:\n",
    "\n",
    "        {state.clarification_message}\n",
    "\n",
    "        ¿Podrías proporcionar esos detalles adicionales para que pueda ayudarte mejor?\n",
    "    \"\"\"\n",
    "\n",
    "    # Añadir el mensaje al estado\n",
    "    state.messages.append({\"role\": \"assistant\", \"content\": clarification_request})\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "# Crear el grafo de estados\n",
    "def create_graph():\n",
    "    \"\"\"\n",
    "    Crea y retorna el grafo de estados para el agente de consultas BigQuery.\n",
    "    \"\"\"\n",
    "    # Inicializar el grafo con el estado AgentState\n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # Añadir los nodos al grafo\n",
    "    graph.add_node(Nodes.EXTRACT_SCHEMA, extract_schema)\n",
    "    graph.add_node(Nodes.VALIDATE_QUESTION, validate_question)\n",
    "    graph.add_node(Nodes.GENERATE_QUERY, generate_query)\n",
    "    graph.add_node(Nodes.VALIDATE_QUERY, validate_query)\n",
    "    graph.add_node(Nodes.EXECUTE_QUERY, execute_query)\n",
    "    graph.add_node(Nodes.PRESENT_RESULTS, present_results)\n",
    "    graph.add_node(Nodes.REQUEST_CLARIFICATION, request_clarification)\n",
    "\n",
    "    # Definir las conexiones entre nodos\n",
    "    graph.add_edge(Nodes.EXTRACT_SCHEMA, Nodes.VALIDATE_QUESTION)\n",
    "    graph.add_conditional_edges(\n",
    "        Nodes.VALIDATE_QUESTION, lambda state: {\"next\": state[\"next\"]}\n",
    "    )\n",
    "    graph.add_edge(Nodes.GENERATE_QUERY, Nodes.VALIDATE_QUERY)\n",
    "    graph.add_conditional_edges(\n",
    "        Nodes.VALIDATE_QUERY, lambda state: {\"next\": state[\"next\"]}\n",
    "    )\n",
    "    graph.add_edge(Nodes.EXECUTE_QUERY, Nodes.PRESENT_RESULTS)\n",
    "    graph.add_edge(Nodes.PRESENT_RESULTS, END)\n",
    "    graph.add_edge(Nodes.REQUEST_CLARIFICATION, END)\n",
    "\n",
    "    # Definir el nodo inicial\n",
    "    graph.set_entry_point(Nodes.EXTRACT_SCHEMA)\n",
    "\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Función principal para ejecutar el agente\n",
    "def run_bigquery_agent(\n",
    "    user_question: str, credentials_path: str = \"/Users/nmlemus/.config/gcloud/application_default_credentials.json\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta el agente de consultas BigQuery con una pregunta del usuario.\n",
    "\n",
    "    Args:\n",
    "        user_question: La pregunta del usuario\n",
    "        credentials_path: Ruta al archivo de credenciales de servicio de GCP\n",
    "\n",
    "    Returns:\n",
    "        La respuesta del agente\n",
    "    \"\"\"\n",
    "    # Configurar la variable de entorno para las credenciales\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "\n",
    "    # Crear el grafo\n",
    "    agent = create_graph()\n",
    "\n",
    "    # Inicializar el estado\n",
    "    initial_state = AgentState(messages=[{\"role\": \"user\", \"content\": user_question}])\n",
    "\n",
    "    # Ejecutar el grafo\n",
    "    final_state = agent.invoke(initial_state)\n",
    "\n",
    "    # Retornar el último mensaje del asistente\n",
    "    assistant_messages = [\n",
    "        msg[\"content\"] for msg in final_state.messages if msg[\"role\"] == \"assistant\"\n",
    "    ]\n",
    "\n",
    "    return assistant_messages[-1] if assistant_messages else \"No se pudo procesar la consulta.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e46c4b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599c9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a065cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\textract_schema(extract_schema)\n",
      "\tvalidate_question(validate_question)\n",
      "\tgenerate_query(generate_query)\n",
      "\tvalidate_query(validate_query)\n",
      "\texecute_query(execute_query)\n",
      "\tpresent_results(present_results)\n",
      "\trequest_clarification(request_clarification)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> extract_schema;\n",
      "\texecute_query --> present_results;\n",
      "\textract_schema --> validate_question;\n",
      "\tgenerate_query --> validate_query;\n",
      "\tpresent_results --> __end__;\n",
      "\trequest_clarification --> __end__;\n",
      "\tvalidate_question -.-> extract_schema;\n",
      "\tvalidate_question -.-> generate_query;\n",
      "\tvalidate_question -.-> validate_query;\n",
      "\tvalidate_question -.-> execute_query;\n",
      "\tvalidate_question -.-> present_results;\n",
      "\tvalidate_question -.-> request_clarification;\n",
      "\tvalidate_question -.-> __end__;\n",
      "\tvalidate_query -.-> extract_schema;\n",
      "\tvalidate_query -.-> validate_question;\n",
      "\tvalidate_query -.-> generate_query;\n",
      "\tvalidate_query -.-> execute_query;\n",
      "\tvalidate_query -.-> present_results;\n",
      "\tvalidate_query -.-> request_clarification;\n",
      "\tvalidate_query -.-> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(agent.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8d50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb65fbd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "MalformedError",
     "evalue": "Service account info was not in the expected format, missing fields token_uri, client_email.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMalformedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      3\u001b[39m     user_question = \u001b[33m\"\u001b[39m\u001b[33m¿Cuáles son los 5 productos más vendidos en el último mes?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mrun_bigquery_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 430\u001b[39m, in \u001b[36mrun_bigquery_agent\u001b[39m\u001b[34m(user_question, credentials_path)\u001b[39m\n\u001b[32m    427\u001b[39m initial_state = AgentState(messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_question}])\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Ejecutar el grafo\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m final_state = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;66;03m# Retornar el último mensaje del asistente\u001b[39;00m\n\u001b[32m    433\u001b[39m assistant_messages = [\n\u001b[32m    434\u001b[39m     msg[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m final_state.messages \u001b[38;5;28;01mif\u001b[39;00m msg[\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    435\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2375\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[39m\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2374\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2375\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2376\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2384\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2031\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2025\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2026\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2027\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2028\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2029\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2030\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2031\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2038\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/langgraph/pregel/runner.py:230\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    228\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/langgraph/utils/runnable.py:546\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    542\u001b[39m config = patch_config(\n\u001b[32m    543\u001b[39m     config, callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    544\u001b[39m )\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    548\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/langgraph/utils/runnable.py:310\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    309\u001b[39m     context.run(_set_config_context, config)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mextract_schema\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_schema\u001b[39m(state: AgentState) -> AgentState:\n\u001b[32m     68\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    Extrae el esquema de la base de datos de BigQuery.\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     client = \u001b[43mget_bigquery_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# Aquí necesitamos definir el dataset a consultar\u001b[39;00m\n\u001b[32m     74\u001b[39m     dataset_id = \u001b[33m\"\u001b[39m\u001b[33myour_dataset_id\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Reemplazar con el ID de tu dataset\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mget_bigquery_client\u001b[39m\u001b[34m(credentials_path)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_bigquery_client\u001b[39m(\n\u001b[32m     31\u001b[39m     credentials_path: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m/Users/nmlemus/.config/gcloud/application_default_credentials.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m ):\n\u001b[32m     33\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m    Inicializa y retorna un cliente de BigQuery.\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     credentials = \u001b[43mservice_account\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCredentials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_service_account_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://www.googleapis.com/auth/bigquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/google/oauth2/service_account.py:260\u001b[39m, in \u001b[36mCredentials.from_service_account_file\u001b[39m\u001b[34m(cls, filename, **kwargs)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_service_account_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, **kwargs):\n\u001b[32m    250\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates a Credentials instance from a service account json file.\u001b[39;00m\n\u001b[32m    251\u001b[39m \n\u001b[32m    252\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m \u001b[33;03m            credentials.\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     info, signer = \u001b[43m_service_account_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_filename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclient_email\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken_uri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_signer_and_info(signer, info, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/google/auth/_service_account_info.py:80\u001b[39m, in \u001b[36mfrom_filename\u001b[39m\u001b[34m(filename, require, use_rsa_signer)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m io.open(filename, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[32m     79\u001b[39m     data = json.load(json_file)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data, \u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rsa_signer\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_rsa_signer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/genai/lib/python3.11/site-packages/google/auth/_service_account_info.py:50\u001b[39m, in \u001b[36mfrom_dict\u001b[39m\u001b[34m(data, require, use_rsa_signer)\u001b[39m\n\u001b[32m     47\u001b[39m missing = keys_needed.difference(data.keys())\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.MalformedError(\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mService account info was not in the expected format, missing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfields \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(missing))\n\u001b[32m     53\u001b[39m     )\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Create a signer.\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_rsa_signer:\n",
      "\u001b[31mMalformedError\u001b[39m: Service account info was not in the expected format, missing fields token_uri, client_email.",
      "During task with name 'Nodes.EXTRACT_SCHEMA' and id '285eb35d-526d-c675-7ad0-7bae1a9d3b29'"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"¿Cuáles son los 5 productos más vendidos en el último mes?\"\n",
    "    response = run_bigquery_agent(user_question)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
